# Regulation-Searching 项目需求文档

## 1. 项目概述

### 1.1 项目简介

Regulation-Searching 是一个基于本地大型语言模型（LLM）和检索增强生成（RAG）技术的企业级法规智能检索系统。本项目旨在为企业提供一个安全、高效、精准的法规查询解决方案，帮助员工快速获取并理解相关法规条文。

### 1.2 项目目标

*   **前端实现**：根据用户提供的Figma设计稿，构建一个现代化、用户友好的前端交互界面。
*   **后端集成**：集成Ollama本地化LLM，确保数据处理的私密性和安全性。
*   **RAG系统**：实现RAG系统，通过引入外部知识库，提高LLM在专业领域的回答准确性。
*   **高效检索**：提供快速、精准的法规信息检索功能。
*   **便捷部署**：支持在用户现有的公司服务器环境下进行部署。

### 1.3 项目范围

本项目主要包含以下几个方面：

*   **前端开发**：基于Figma设计稿，使用React框架开发前端应用。
*   **后端开发**：使用Python语言及相关框架（如FastAPI）开发后端服务，包括API接口、Ollama集成、RAG系统实现等。
*   **数据库设计**：设计并实现用于存储法规文档、用户查询历史等数据的数据库。
*   **部署方案**：提供详细的部署文档，指导用户在公司服务器上完成项目的部署和配置。

## 2. 功能需求

### 2.1 用户界面 (UI)

前端界面严格遵循用户提供的Figma设计稿，实现一个类似于ChatGPT的对话式UI。主要包含以下几个部分：

*   **对话列表**：左侧边栏显示历史对话列表，方便用户快速切换和回顾。
*   **对话窗口**：主窗口区域用于显示用户与AI的对话内容。
*   **输入区域**：底部提供文本输入框，供用户输入问题。
*   **功能按钮**：提供新建对话、清除历史记录、设置等功能按钮。

### 2.2 核心功能

| 功能模块 | 功能描述 |
| :--- | :--- |
| **用户认证** | （可选）提供简单的用户登录和认证功能，用于区分不同用户的数据。 |
| **法规检索** | 用户在输入框中输入问题，系统通过RAG技术从法规知识库中检索相关信息，并由LLM生成回答。 |
| **对话管理** | 支持创建新的对话、保存历史对话、删除指定对话等功能。 |
| **知识库管理** | （后台功能）提供一个简单的管理界面，用于上传、更新、删除法规文档，并对文档进行向量化处理。 |

## 3. 非功能性需求

| 需求类别 | 具体要求 |
| :--- | :--- |
| **性能** | - **响应时间**：用户查询的平均响应时间应在5秒以内。<br>- **并发处理**：系统应能支持至少10个用户的并发查询。 |
| **安全性** | - **数据私密性**：所有数据（包括法规文档、用户查询）均存储在本地服务器，不与任何外部云服务交互。<br>- **访问控制**：后台管理界面需要进行身份验证。 |
| **可扩展性** | 系统架构应具备良好的可扩展性，方便未来增加新的功能模块或集成其他系统。 |
| **易用性** | - **前端界面**：界面简洁直观，操作便捷。<br>- **后台管理**：后台管理功能清晰明了，易于操作。 |

## 4. 系统架构

### 4.1 整体架构

系统采用前后端分离的架构，主要由以下几部分组成：

1.  **前端应用 (Frontend)**：基于React的单页面应用（SPA），负责用户界面的展示和交互。
2.  **后端服务 (Backend)**：基于Python FastAPI的API服务，负责处理业务逻辑，包括用户请求处理、与LLM和RAG系统的交互等。
3.  **Ollama LLM服务**：本地部署的大型语言模型服务，提供文本生成能力。
4.  **RAG系统**：负责从知识库中检索与用户查询最相关的信息。
5.  **向量数据库 (Vector DB)**：用于存储法规文档的向量表示，实现快速检索。
6.  **关系型数据库 (Database)**：用于存储用户信息、对话历史等结构化数据。

### 4.2 技术栈

| 领域 | 技术选型 |
| :--- | :--- |
| **前端** | React, TypeScript, Tailwind CSS |
| **后端** | Python, FastAPI |
| **LLM** | Ollama (例如，使用Llama3或Qwen2等模型) |
| **RAG框架** | LangChain 或 LlamaIndex |
| **向量数据库** | ChromaDB 或 Faiss |
| **关系型数据库** | SQLite 或 PostgreSQL |
| **Web服务器** | Uvicorn |
| **容器化** | Docker, Docker Compose |

## 5. 部署方案

### 5.1 部署环境

*   **服务器操作系统**：Windows Server 2019 (在WSL2 Ubuntu中部署)
*   **依赖软件**：Docker, Docker Compose, Python, Node.js

### 5.2 部署步骤

1.  **准备环境**：在WSL2 Ubuntu中安装Docker和Docker Compose。
2.  **克隆代码**：从GitHub仓库克隆项目代码。
3.  **构建镜像**：使用`docker-compose build`命令构建前端和后端的Docker镜像。
4.  **启动服务**：使用`docker-compose up -d`命令启动所有服务，包括前端、后端、Ollama、数据库等。
5.  **配置Ollama**：首次启动时，需要进入Ollama容器下载所需的LLM模型。
6.  **初始化知识库**：通过后台管理界面或命令行工具，上传法规文档并进行向量化处理。

## 6. 项目计划

| 阶段 | 主要任务 | 预计时间 |
| :--- | :--- | :--- |
| **第一阶段** | - 需求确认与技术选型<br>- 搭建项目基本框架<br>- 完成GitHub仓库的创建和初始化 | 1-2天 |
| **第二阶段** | - **后端开发**：<br>  - API接口开发<br>  - Ollama与RAG系统集成<br>- **前端开发**：<br>  - 根据Figma搭建UI框架<br>  - 实现核心对话功能 | 1周 |
| **第三阶段** | - 前后端联调<br>- 知识库管理功能开发<br>- 单元测试与集成测试 | 1周 |
| **第四阶段** | - 部署文档编写<br>- 系统部署与测试<br>- 用户培训与项目交付 | 1-2天 |

---

**文档版本**: 1.0
**创建日期**: 2025年10月20日
**作者**: Manus AI

